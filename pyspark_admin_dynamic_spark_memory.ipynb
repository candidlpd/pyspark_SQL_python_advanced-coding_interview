{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"H:\\pyspark_SQL_python_advanced-coding_interview\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c:\\\\Users\\\\lpdda\\\\.vscode\\\\extensions\\\\ms-python.python-2024.22.0-win32-x64\\\\python_files', 'C:\\\\spark\\\\python', 'C:\\\\Users\\\\lpdda\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python311.zip', 'C:\\\\Users\\\\lpdda\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\DLLs', 'C:\\\\Users\\\\lpdda\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib', 'C:\\\\Users\\\\lpdda\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311', 'C:\\\\Users\\\\lpdda\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages', 'C:\\\\Users\\\\lpdda\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\win32', 'C:\\\\Users\\\\lpdda\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\lpdda\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\Pythonwin', 'C:\\\\Users\\\\lpdda\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib\\\\site-packages']\n",
       "C:\\Users\\lpdda\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\n",
       "3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, sys \n",
    "print(sys.path)\n",
    "print(sys.executable)\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Python 3.11.8\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import subprocess \n",
    "result = subprocess.run([\"python\", \"--version\"], capture_output=True, text = True)\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Is Root user: False\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#root user check\n",
    "is_root = os.getguid() == 0 if hasattr(os, \"getguid\") else False \n",
    "print(f\"Is Root user: {is_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hostname : LPD\n",
       "Operating System: Windows\n",
       "Platform Details: Windows-10-10.0.22631-SP0\n",
       "Machine Architecture: AMD64\n",
       "Processor: Intel64 Family 6 Model 154 Stepping 3, GenuineIntel\n",
       "Python Version: 3.11.8\n",
       "Python Implementation: CPython\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hostname \n",
    "import platform \n",
    "hostname = platform.node()\n",
    "print(f\"hostname : {hostname}\")\n",
    "\n",
    "# OS and Hardware Information\n",
    "print(f\"Operating System: {platform.system()}\")\n",
    "print(f\"Platform Details: {platform.platform()}\")\n",
    "print(f\"Machine Architecture: {platform.machine()}\")\n",
    "print(f\"Processor: {platform.processor()}\")\n",
    "\n",
    "# Python Information\n",
    "print(f\"Python Version: {platform.python_version()}\")\n",
    "print(f\"Python Implementation: {platform.python_implementation()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.1.0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import psutil\n",
    "print(psutil.__version__)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark application name: interview questions\n",
       "spark version: 3.4.3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"interview questions\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(f\"spark application name: {sc.appName}\")\n",
    "print(f\"spark version: {sc.version}\")\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bydefault on pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark application name: interview questions\n",
       "spark version: 3.4.3\n",
       "Number of Executors: 1\n",
       "Number of Worker Nodes: 1\n",
       "Number of Spark Drivers: 1\n",
       "Driver Memory: 1g\n",
       "Executor Memory: 1g\n",
       "Worker Node Memory: 31.72 GB\n",
       "Spark Cluster Manager: local[*]\n",
       "Spark UI URL: http://localhost:4040\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"interview questions\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(f\"spark application name: {sc.appName}\")\n",
    "print(f\"spark version: {sc.version}\")\n",
    "\n",
    "# Number of Executors\n",
    "num_executors = int(sc.getConf().get(\"spark.executor.instances\", \"1\"))\n",
    "print(f\"Number of Executors: {num_executors}\")\n",
    "\n",
    "# Number of Worker Nodes\n",
    "worker_nodes = 1 if \"local\" in sc.master else \"Check Cluster Config\"\n",
    "print(f\"Number of Worker Nodes: {worker_nodes}\")\n",
    "\n",
    "# Number of Spark Drivers\n",
    "print(\"Number of Spark Drivers: 1\")\n",
    "\n",
    "# Driver Memory\n",
    "driver_memory = sc.getConf().get(\"spark.driver.memory\", \"1g\")\n",
    "print(f\"Driver Memory: {driver_memory}\")\n",
    "\n",
    "# Executors Memory\n",
    "executor_memory = sc.getConf().get(\"spark.executor.memory\", \"1g\")\n",
    "print(f\"Executor Memory: {executor_memory}\")\n",
    "\n",
    "# Worker Node Memory\n",
    "worker_node_memory = psutil.virtual_memory().total / (1024 ** 3)\n",
    "print(f\"Worker Node Memory: {worker_node_memory:.2f} GB\")\n",
    "\n",
    "# Spark Cluster Manager\n",
    "cluster_manager = sc.master\n",
    "print(f\"Spark Cluster Manager: {cluster_manager}\")\n",
    "\n",
    "# Spark UI URL\n",
    "spark_ui_url = sc.uiWebUrl\n",
    "print(f\"Spark UI URL: {spark_ui_url}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic calculation of all the memory and cpu configurations in system and spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Total Hard Disk Size: 195.31 GB\n",
       "\n",
       "Total RAM Memory: 31.72 GB\n",
       "Reserved Memory (OS + Cluster Manager): 3.00 GB\n",
       "\n",
       "Formula: Available Memory = Total RAM - Reserved Memory\n",
       "Available Memory for Spark: 28.72 GB\n",
       "\n",
       "Total CPU Cores: 20\n",
       "\n",
       "Formula: Number of Executors = Total Cores // Cores per Executor\n",
       "Number of Executors (with 4 cores each): 5\n",
       "\n",
       "\n",
       "Formula: Executor Memory = Available Memory / Number of Executors\n",
       "Memory per Executor: 5.74 GB\n",
       "\n",
       "Formula: On-Heap Memory = Executor Memory * 0.8\n",
       "On-Heap Memory per Executor: 4.59 GB\n",
       "\n",
       "Formula: Off-Heap Memory = Executor Memory * 0.2\n",
       "Off-Heap Memory (Shared): 1.15 GB\n",
       "\n",
       "Formula: GC Memory = On-Heap Memory * 0.1\n",
       "GC Memory per Executor: 0.46 GB\n",
       "\n",
       "Formula: Spark Execution Memory = On-Heap Memory * 0.6\n",
       "Spark Execution Memory: 2.76 GB\n",
       "\n",
       "Formula: Spark Storage Memory = On-Heap Memory * 0.4\n",
       "Spark Storage Memory: 1.84 GB\n",
       "\n",
       "Formula: User Memory = Executor Memory * 0.1\n",
       "User Memory per Executor: 0.57 GB\n",
       "\n",
       "Reserved Memory per Executor: 0.30 GB\n",
       "\n",
       "Formula: Overhead Memory = Executor Memory * 0.07\n",
       "Overhead Memory per Executor: 0.40 GB\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import psutil\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DynamicMemoryCalculationWithFormula\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Fetch system configurations dynamically\n",
    "total_ram = psutil.virtual_memory().total / (1024 ** 3)  # Convert to GB\n",
    "total_cores = psutil.cpu_count(logical=True)  # Total logical CPU cores\n",
    "os_memory = 2  # Reserved for OS in GB\n",
    "cluster_manager_memory = 1  # Reserved for Cluster Manager in GB\n",
    "reserved_memory = os_memory + cluster_manager_memory\n",
    "\n",
    "# Total Hard Disk size in GB\n",
    "total_disk = shutil.disk_usage('/').total / (1024 ** 3)\n",
    "print(f\"Total Hard Disk Size: {total_disk:.2f} GB\\n\")\n",
    "\n",
    "# Available memory for Spark\n",
    "available_memory = total_ram - reserved_memory\n",
    "print(f\"Total RAM Memory: {total_ram:.2f} GB\")\n",
    "print(f\"Reserved Memory (OS + Cluster Manager): {reserved_memory:.2f} GB\")\n",
    "print(\"\\nFormula: Available Memory = Total RAM - Reserved Memory\")\n",
    "print(f\"Available Memory for Spark: {available_memory:.2f} GB\\n\")\n",
    "\n",
    "# Dynamically calculate number of executors based on cores per executor\n",
    "cores_per_executor = 4  # Change to 5 if needed\n",
    "num_executors = total_cores // cores_per_executor\n",
    "remaining_cores = total_cores % cores_per_executor\n",
    "print(f\"Total CPU Cores: {total_cores}\")\n",
    "print(\"\\nFormula: Number of Executors = Total Cores // Cores per Executor\")\n",
    "print(f\"Number of Executors (with {cores_per_executor} cores each): {num_executors}\")\n",
    "if remaining_cores > 0:\n",
    "    print(f\"Remaining Cores (unused): {remaining_cores}\\n\")\n",
    "print('\\n')\n",
    "\n",
    "# Memory allocation per executor\n",
    "executor_memory = available_memory / num_executors\n",
    "print(\"Formula: Executor Memory = Available Memory / Number of Executors\")\n",
    "print(f\"Memory per Executor: {executor_memory:.2f} GB\\n\")\n",
    "\n",
    "# Memory Distribution\n",
    "on_heap_memory = executor_memory * 0.8\n",
    "off_heap_memory = executor_memory * 0.2\n",
    "gc_memory = on_heap_memory * 0.1  # GC is 10% of on-heap memory\n",
    "print(\"Formula: On-Heap Memory = Executor Memory * 0.8\")\n",
    "print(f\"On-Heap Memory per Executor: {on_heap_memory:.2f} GB\\n\")\n",
    "print(\"Formula: Off-Heap Memory = Executor Memory * 0.2\")\n",
    "print(f\"Off-Heap Memory (Shared): {off_heap_memory:.2f} GB\\n\")\n",
    "print(\"Formula: GC Memory = On-Heap Memory * 0.1\")\n",
    "print(f\"GC Memory per Executor: {gc_memory:.2f} GB\\n\")\n",
    "\n",
    "# Spark Memory Allocation\n",
    "spark_execution_memory = on_heap_memory * 0.6  # Execution memory (60%)\n",
    "spark_storage_memory = on_heap_memory * 0.4  # Storage memory (40%)\n",
    "user_memory = executor_memory * 0.1  # User-defined memory (10%)\n",
    "reserved_memory_executor = 0.3  # Reserved (~300 MB)\n",
    "overhead_memory = executor_memory * 0.07  # Default overhead (7%)\n",
    "print(\"Formula: Spark Execution Memory = On-Heap Memory * 0.6\")\n",
    "print(f\"Spark Execution Memory: {spark_execution_memory:.2f} GB\\n\")\n",
    "print(\"Formula: Spark Storage Memory = On-Heap Memory * 0.4\")\n",
    "print(f\"Spark Storage Memory: {spark_storage_memory:.2f} GB\\n\")\n",
    "print(\"Formula: User Memory = Executor Memory * 0.1\")\n",
    "print(f\"User Memory per Executor: {user_memory:.2f} GB\\n\")\n",
    "print(f\"Reserved Memory per Executor: {reserved_memory_executor:.2f} GB\\n\")\n",
    "print(\"Formula: Overhead Memory = Executor Memory * 0.07\")\n",
    "print(f\"Overhead Memory per Executor: {overhead_memory:.2f} GB\\n\")\n",
    "\n",
    "# Set Spark configurations dynamically\n",
    "sc.getConf().set(\"spark.executor.instances\", str(num_executors))\n",
    "sc.getConf().set(\"spark.executor.memory\", f\"{executor_memory:.2f}g\")\n",
    "sc.getConf().set(\"spark.executor.cores\", str(cores_per_executor))\n",
    "sc.getConf().set(\"spark.memory.offHeap.enabled\", \"true\")\n",
    "sc.getConf().set(\"spark.memory.offHeap.size\", f\"{off_heap_memory:.2f}g\")\n",
    "\n",
    "# Stop Spark\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
