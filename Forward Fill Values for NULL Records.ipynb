{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H:\\pyspark_advanced-coding_interview\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"H:\\pyspark_advanced-coding_interview\")\n",
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session with optimized settings\n",
    "spark = (\n",
    "    SparkSession.builder \n",
    "    .appName(\"OptimizedLocalSpark\") \n",
    "    .config(\"spark.driver.memory\", \"8g\")        \n",
    "    .config(\"spark.executor.memory\", \"8g\")    \n",
    "    .config(\"spark.executor.cores\", \"4\")       \n",
    "    .config(\"spark.cores.max\", \"12\")           \n",
    "    .config(\"spark.sql.shuffle.partitions\", \"28\")  \n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \n",
    "    .getOrCreate()\n",
    ")\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------------------+\n",
      "|employee_id|      date|          logindate|\n",
      "+-----------+----------+-------------------+\n",
      "|          1|2024-10-01|               null|\n",
      "|          1|2024-10-02|               null|\n",
      "|          1|2024-10-03|2024-10-03 08:00:00|\n",
      "|          1|2024-10-04|               null|\n",
      "|          1|2024-10-05|               null|\n",
      "|          2|2024-10-01|               null|\n",
      "|          2|2024-10-02|2024-10-02 09:00:00|\n",
      "|          2|2024-10-03|               null|\n",
      "|          2|2024-10-04|               null|\n",
      "|          3|2024-10-01|               null|\n",
      "|          3|2024-10-02|               null|\n",
      "|          3|2024-10-03|2024-10-03 10:00:00|\n",
      "+-----------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ForwardFillNULLValues\").getOrCreate()\n",
    "\n",
    "# Sample data with NULL values in the logindate column\n",
    "data = [\n",
    "    (1, '2024-10-01', None),\n",
    "    (1, '2024-10-02', None),\n",
    "    (1, '2024-10-03', '2024-10-03 08:00:00'),\n",
    "    (1, '2024-10-04', None),\n",
    "    (1, '2024-10-05', None),\n",
    "    (2, '2024-10-01', None),\n",
    "    (2, '2024-10-02', '2024-10-02 09:00:00'),\n",
    "    (2, '2024-10-03', None),\n",
    "    (2, '2024-10-04', None),\n",
    "    (3, '2024-10-01', None),\n",
    "    (3, '2024-10-02', None),\n",
    "    (3, '2024-10-03', '2024-10-03 10:00:00'),\n",
    "]\n",
    "\n",
    "columns = [\"employee_id\", \"date\", \"logindate\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Create temporary table for SQL queries\n",
    "df.createOrReplaceTempView(\"employee_logins\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------------------+\n",
      "|employee_id|      date|   filled_logindate|\n",
      "+-----------+----------+-------------------+\n",
      "|          1|2024-10-01|               null|\n",
      "|          1|2024-10-02|               null|\n",
      "|          1|2024-10-03|2024-10-03 08:00:00|\n",
      "|          1|2024-10-04|2024-10-03 08:00:00|\n",
      "|          1|2024-10-05|2024-10-03 08:00:00|\n",
      "|          2|2024-10-01|               null|\n",
      "|          2|2024-10-02|2024-10-02 09:00:00|\n",
      "|          2|2024-10-03|2024-10-02 09:00:00|\n",
      "|          2|2024-10-04|2024-10-02 09:00:00|\n",
      "|          3|2024-10-01|               null|\n",
      "|          3|2024-10-02|               null|\n",
      "|          3|2024-10-03|2024-10-03 10:00:00|\n",
      "+-----------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res_sql = spark.sql(\"\"\"\n",
    "SELECT employee_id, \n",
    "       date, \n",
    "       COALESCE(logindate, LAST_VALUE(logindate, TRUE) OVER (\n",
    "           PARTITION BY employee_id ORDER BY date \n",
    "           ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
    "       )) AS filled_logindate\n",
    "FROM employee_logins\n",
    "ORDER BY employee_id, date\n",
    "\"\"\")\n",
    "res_sql.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------------------+-------------------+\n",
      "|employee_id|      date|          logindate|   filled_logindate|\n",
      "+-----------+----------+-------------------+-------------------+\n",
      "|          1|2024-10-01|               null|               null|\n",
      "|          1|2024-10-02|               null|               null|\n",
      "|          1|2024-10-03|2024-10-03 08:00:00|2024-10-03 08:00:00|\n",
      "|          1|2024-10-04|               null|2024-10-03 08:00:00|\n",
      "|          1|2024-10-05|               null|2024-10-03 08:00:00|\n",
      "|          2|2024-10-01|               null|               null|\n",
      "|          2|2024-10-02|2024-10-02 09:00:00|2024-10-02 09:00:00|\n",
      "|          2|2024-10-03|               null|2024-10-02 09:00:00|\n",
      "|          2|2024-10-04|               null|2024-10-02 09:00:00|\n",
      "|          3|2024-10-01|               null|               null|\n",
      "|          3|2024-10-02|               null|               null|\n",
      "|          3|2024-10-03|2024-10-03 10:00:00|2024-10-03 10:00:00|\n",
      "+-----------+----------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define window specification\n",
    "window_spec = Window.partitionBy(\"employee_id\").orderBy(\"date\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "# Use `last` function with `ignoreNulls=True` to forward-fill NULL values\n",
    "df_filled = df.withColumn(\"filled_logindate\", F.last(\"logindate\", ignorenulls=True).over(window_spec))\n",
    "df_filled.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------------------+-------------------+\n",
      "|employee_id|      date|          logindate|   filled_logindate|\n",
      "+-----------+----------+-------------------+-------------------+\n",
      "|          1|2024-10-01|               null|               null|\n",
      "|          1|2024-10-02|               null|               null|\n",
      "|          1|2024-10-03|2024-10-03 08:00:00|2024-10-03 08:00:00|\n",
      "|          1|2024-10-04|               null|2024-10-03 08:00:00|\n",
      "|          1|2024-10-05|               null|2024-10-03 08:00:00|\n",
      "|          2|2024-10-01|               null|               null|\n",
      "|          2|2024-10-02|2024-10-02 09:00:00|2024-10-02 09:00:00|\n",
      "|          2|2024-10-03|               null|2024-10-02 09:00:00|\n",
      "|          2|2024-10-04|               null|2024-10-02 09:00:00|\n",
      "|          3|2024-10-01|               null|               null|\n",
      "|          3|2024-10-02|               null|               null|\n",
      "|          3|2024-10-03|2024-10-03 10:00:00|2024-10-03 10:00:00|\n",
      "+-----------+----------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filled = df.withColumn(\"filled_logindate\", \n",
    "                          F.last(\"logindate\", ignorenulls=True).over(window_spec))\n",
    "df_filled.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+\n",
      "|CurrencyKey| DateKey|  Rate|\n",
      "+-----------+--------+------+\n",
      "|          3|20201229|0.9998|\n",
      "|          3|20201230|1.0009|\n",
      "|          3|20201231|0.9108|\n",
      "|          3|20210101|  null|\n",
      "|          3|20210102|  null|\n",
      "|          3|20210103|  null|\n",
      "|          4|20201229|  0.85|\n",
      "|          4|20201230|  0.86|\n",
      "|          4|20201231|  null|\n",
      "|          4|20210101|  null|\n",
      "|          4|20210102|  0.87|\n",
      "|          4|20210103|  null|\n",
      "+-----------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CurrencyRateGrp\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (3, 20201229, 0.9998),\n",
    "    (3, 20201230, 1.0009),\n",
    "    (3, 20201231, 0.9108),\n",
    "    (3, 20210101, None),\n",
    "    (3, 20210102, None),\n",
    "    (3, 20210103, None),\n",
    "    (4, 20201229, 0.8500),\n",
    "    (4, 20201230, 0.8600),\n",
    "    (4, 20201231, None),\n",
    "    (4, 20210101, None),\n",
    "    (4, 20210102, 0.8700),\n",
    "    (4, 20210103, None)\n",
    "]\n",
    "\n",
    "columns = [\"CurrencyKey\", \"DateKey\", \"Rate\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Create a temporary view for SQL usage\n",
    "df.createOrReplaceTempView(\"CurrencyRate\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+---+------------+\n",
      "|CurrencyKey| DateKey|  Rate|Grp|currencyRate|\n",
      "+-----------+--------+------+---+------------+\n",
      "|          3|20201229|0.9998|  1|      0.9998|\n",
      "|          3|20201230|1.0009|  2|      1.0009|\n",
      "|          3|20201231|0.9108|  3|      0.9108|\n",
      "|          3|20210101|  null|  3|      0.9108|\n",
      "|          3|20210102|  null|  3|      0.9108|\n",
      "|          3|20210103|  null|  3|      0.9108|\n",
      "|          4|20201229|  0.85|  1|        0.85|\n",
      "|          4|20201230|  0.86|  2|        0.86|\n",
      "|          4|20201231|  null|  2|        0.86|\n",
      "|          4|20210101|  null|  2|        0.86|\n",
      "|          4|20210102|  0.87|  3|        0.87|\n",
      "|          4|20210103|  null|  3|        0.87|\n",
      "+-----------+--------+------+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res_sql = spark.sql(\"\"\"\n",
    "                    \n",
    "with currencyGrp as (\n",
    "SELECT \n",
    "    CurrencyKey,\n",
    "    DateKey,\n",
    "    Rate,\n",
    "    COUNT(Rate) OVER (PARTITION BY CurrencyKey ORDER BY DateKey) AS Grp\n",
    "FROM CurrencyRate\n",
    "ORDER BY CurrencyKey, DateKey\n",
    ")\n",
    "\n",
    "select *,\n",
    "max(Rate) over (partition by CurrencyKey, Grp order by DateKey) as currencyRate\n",
    "from currencyGrp \n",
    "\"\"\")\n",
    "res_sql.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+---+\n",
      "|CurrencyKey| DateKey|  Rate|Grp|\n",
      "+-----------+--------+------+---+\n",
      "|          3|20201229|0.9998|  1|\n",
      "|          3|20201230|1.0009|  2|\n",
      "|          3|20201231|0.9108|  3|\n",
      "|          3|20210101|  null|  3|\n",
      "|          3|20210102|  null|  3|\n",
      "|          3|20210103|  null|  3|\n",
      "|          4|20201229|  0.85|  1|\n",
      "|          4|20201230|  0.86|  2|\n",
      "|          4|20201231|  null|  2|\n",
      "|          4|20210101|  null|  2|\n",
      "|          4|20210102|  0.87|  3|\n",
      "|          4|20210103|  null|  3|\n",
      "+-----------+--------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define window specification for cumulative count of non-null Rate within each CurrencyKey partition\n",
    "window_spec = Window.partitionBy(\"CurrencyKey\").orderBy(\"DateKey\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "# Use count function with the window to calculate Grp\n",
    "df_with_grp = df.withColumn(\"Grp\", F.count(\"Rate\").over(window_spec))\n",
    "df_with_grp.orderBy(\"CurrencyKey\", \"DateKey\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
